{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"8b9d6583-6ae1-4bb6-970c-c80d42dfebdb","_uuid":"d56b7961b91d7f67554ec48bd1d2485fb7419120"},"source":["# Sentence categorisation using TF-IDF with k-means\n","\n","The dataset was found here: https://wortschatz.uni-leipzig.de/en/download/French\n","It contains a mix of one million french sentences from various sources.\n"]},{"cell_type":"markdown","metadata":{},"source":["The app will have many sentences in the target language that the user will see one after the other in increasing difficulty order. However, it would be nice if the user had the option to filter down the sentences they see to certain categories. For example, if they would like to practice vocabulary specific to the home, or sports. Instead of manually categorising what will eventually be thousands of sentences, I need to create a ml model to automatically place new sentences into a category as accurately as possible.\n","\n","There are a few supervised methods I could use such as naive bayes, support vector machines as well as deep learning models. However, I'll probably get the best results by using google's pre-trained BERT model as a base to encode the sentences and training a few fully-connected layers at the end for my custom classifyer. Computational cost is not so much of a concern as I'll only be running the classifier once on the dataset to generate a category column for the database. Although BERT was originally trained on an english corpus, there's another model called Multilingual BERT that has been trained on Wikipedia in 104 different languages, which would be well-suited to this application.\n","\n","However, I found it pretty hard to find a labeled ground-truth dataset for sentence-level subject classification, especially for foreign languages. I could create one myself but making an unbiased labelled dataset with enough samples in each category would be pretty difficult and probably not worth the time investment in this case, so in the end I've decided to use an unsupervised approach and infer categories. I need to pick between clustering and topic modelling.\n","\n","For this application, I want the sentences to be clustered by topic and not something else like sentiment. Representing the sentences using a bag-of-words model would group sentences together based on the words they contain, which would be more likely to result in topic-based clusters. Better yet, I could use TF-IDF, which, like bag-of-words, assigns a score based on the number of times a word appears in the document but is counterbalanced by the number of sentences in which it is present. Words like ‚Äòthis‚Äô, ‚Äôare‚Äô, that are commonly present in all the sentences are not given a very high rank. However, a word that is present too many times in a few of the sentences will be given a higher rank as it might be indicative of its context.\n","\n","Once I have fit the k-means model I can use it to predict which category new sentences belong to."]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":false},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'seaborn'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Toby Usher\\Documents\\dev\\quivo-app\\ml_notebooks\\sentence_categorisation\\k_means_clustering_of_french_sentences.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#from sklearn.feature_extraction import text\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfVectorizer\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"]}],"source":["import os\n","import re\n","import math\n","import numpy as np\n","import pandas as pd \n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","#from sklearn.feature_extraction import text\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.stem.snowball import SnowballStemmer\n","%matplotlib inline\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#filepath = os.path.join(\"./fra_mixed_2009_1M\", \"fra_mixed_2009_1M-sentences.txt\")\n","\n","# Train on cleaned dataset of around 850,000 french sentences, capped to 200 characters max\n","filepath = os.path.join(\"..\", \"french_sentences.csv\")\n","\n","df = pd.read_csv(filepath, delimiter='\\t', header=None)\n","df.columns = [\"id\", \"sentence\"]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Cleaning the data"]},{"cell_type":"markdown","metadata":{"_cell_guid":"ff887c6d-0470-4f62-860b-9457b223bb8c","_uuid":"eb590852f097f66ea53be9a970789430fc3f6a63"},"source":["Delete any potential duplicates"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Let's see if there are any duplicates in the dataset\n","df[df[\"sentence\"].duplicated(keep=False)].sort_values(\"sentence\").head(8)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Remove all duplicates from the dataframe\n","df = df.drop_duplicates(\"sentence\")"]},{"cell_type":"markdown","metadata":{"_cell_guid":"f1bb8d35-27aa-4ff4-9a39-4329517aa6a4","_uuid":"f022fdf6441499ed52b34c063240f4f28b2ff3a5"},"source":["# NLP "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# First we need to get a list of stop words in french. These are common filler words that provide almost no useful information\n","# and occur frequently in most documents. Stop words can have a disproportionate influence on the overall representation of\n","# the document, which can be detrimental to the performance of TF-IDF. To mitigate this we need to remove stop words before\n","# calculating TF-IDF vectors\n","\n","punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\n","\n","# Get French stop words\n","french_stop_words = stopwords.words('french')\n","\n","# Combine with standard punctuation to get the comprehensive list of stop words. Convert stop words to a set\n","# to ensure no duplicates\n","stop_words = set(french_stop_words).union(punc)\n","\n","# convert back to list\n","stop_words = list(stop_words)\n","\n","print(stop_words)"]},{"cell_type":"markdown","metadata":{},"source":["There are lots of possible numerical values that can occur within a text, which means that a large fraction of the unique values could be numerical or at least contain strings of numbers. This can add noise to the data and lead to a sparse dataset and affect model performance, so I use a custom token pattern that ignores all words containing any numerical value. I might modify this in the future as it excludes important information such as dates, but as an initial proof-of-concept it's probably not too much of an issue."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get the raw sentences from the dataframe\n","sentences = df['sentence'].values\n","\n","# Create a custom token pattern that matches words containing only letters\n","# The pattern \\b\\w+\\b matches whole words, and [^\\W\\d_] matches any character that is not a non-word character, digit, or underscore\n","token_pattern = r'\\b[^\\W\\d_]+\\b'\n","\n","# Create the TF-IDF vectorizer object and fit to our dataset to generate vector encodings for each sentence\n","vectorizer = TfidfVectorizer(stop_words=stop_words, token_pattern=token_pattern)\n","X = vectorizer.fit_transform(sentences)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c7e595ab-440c-4ad7-98e4-4358cc724d8c","_uuid":"9c1c23ecabae8217a9aa8f90371f2a30053cc6f1","collapsed":true,"trusted":false},"outputs":[],"source":["punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\n","#stop_words = text.ENGLISH_STOP_WORDS.union(punc)\n","desc = data['headline_text'].values\n","vectorizer = TfidfVectorizer(stop_words = stop_words)\n","X = vectorizer.fit_transform(desc)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eb56971e-5412-4138-a4cd-0e14844796be","_uuid":"10af64e15e2f08c30da71b847432eedc2aece199","collapsed":true,"trusted":false},"outputs":[],"source":["word_features = vectorizer.get_feature_names_out()\n","# Show the unique words that occurred in the training data\n","print(len(word_features))\n","print(word_features[:100])"]},{"cell_type":"markdown","metadata":{},"source":["It looks like the data could be further cleaned. For example, I could remove words with three or more of the same letter in a row. For now I'll leave it but I'll probably come back and properly clean the data once I have a preliminary output."]},{"cell_type":"markdown","metadata":{"_cell_guid":"871b1bd6-c411-4ff6-a784-9b376a0db4e6","_uuid":"e480f5b88938660f05c09f75af5f9f58d7110096"},"source":["# Stemming and Tokenizing\n","Stemming is the process of reducing a word into its stem, i.e. its root form. The root form is not necessarily a word by itself, but it can be used to generate words by concatenating the right suffix. For example, the words fish, fishes and fishing all stem into fish, which is a correct word. On the other side, the words study, studies and studying stems into studi, which is not an English word.\n","\n","Tokenization is breaking the sentence into words and punctuation,"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"536a1a88-48a3-43d0-b368-ccf31947e5b1","_uuid":"5d25104db183624b990a1d64e10cc618fd8ee715","collapsed":true,"trusted":false},"outputs":[],"source":["stemmer = SnowballStemmer('english')\n","tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n","\n","def tokenize(text):\n","    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["stemmer = SnowballStemmer('french')\n","tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n","\n","def _tokenize(text: str) -> list[str]:\n","    \"\"\"Tokenizes a document into its individual words and punctuation and returns a list of each token's stem.\n","    \"\"\"\n","\n","    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]"]},{"cell_type":"markdown","metadata":{"_cell_guid":"9877cf33-ebe8-46e4-b26e-c6f673617517","_uuid":"d190c4d6b9cb52ad70a021e3475704538b47f85f"},"source":["# Vectorization with stop words(words irrelevant to the model), stemming and tokenizing"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"18e1d30c-5515-4c0d-89e4-6af99658bee3","_uuid":"5fcc93fa3093f30d181ffa38b4ca46b133316952","collapsed":true,"scrolled":false,"trusted":false},"outputs":[],"source":["vectorizer2 = TfidfVectorizer(stop_words = stop_words, tokenizer = _tokenize)\n","X2 = vectorizer2.fit_transform(sentences)\n","\n","word_features2 = vectorizer2.get_feature_names_out()\n","print(len(word_features2))\n","print(word_features2[:100]) "]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"acdc11f4-7b5c-4aee-8c42-1a752bffbc2e","_uuid":"244a3015e5b4f4f84c174586fa875f5cf49cff1d","collapsed":true,"trusted":false},"outputs":[],"source":["vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = _tokenize, max_features = 1000) # limit to 1000 most frequent terms in the corpus\n","X3 = vectorizer3.fit_transform(sentences)\n","words = vectorizer3.get_feature_names_out()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"e8a8c1ed-8970-49e0-bae5-0e0d85abea84","_uuid":"b5f2e66c25d17527b78ae1a1fe174f1ac6310286"},"source":["# K-means clustering"]},{"cell_type":"markdown","metadata":{"_cell_guid":"c9a1312e-45f7-44e4-9407-13012bdf97ce","_uuid":"39ba3fa0f53454111495da2f7e7719572afec93a"},"source":["We can choose the appropriate number of clusters with the elbow method, which just plots the percentage of variance explained (ratio of the between-group variance to the total variance) as a function of the number of clusters. The appropriate cluster number is one where adding another cluster doesn't give much better modelling of the data i.e. the marginal gains of adding more clusters begins to drop off and doesn't provide a significantly better classification of the data. This point can be identified by an angle (elbow) in the graph, although this point isn't always easy to identify.\n","\n","A rule of thumb to estimate the value of k to choose is ùëò‚àºSQRT(n/2) where n is the number of points to be clustered."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get the number of data points to be clustered\n","num_data_points = X3.shape[0]\n","recommended_k = math.ceil(math.sqrt(num_data_points/2))\n","\n","print(f\"The recommended value of k when clustering {num_data_points} data points is {recommended_k}\")"]},{"cell_type":"markdown","metadata":{},"source":["This is obviously far greater than the 10-15 sentence topic categories I would like to use in my app, so the elbow method will be ineffective as choosing values of k between say five and 20 should result in a relatively inverse linear relationship between k and wcss (within-cluster sum of squares).\n","\n","The purpose of the topic filter is to choose sentences that strongly match a specific topic, and choosing low k values will lead to extremely general categories, which is obviously not what I want.\n","\n","One solution is to choose a high value for k, say several hundred, and find clusters that are strongly correlated with a particular topic that I want to include as a filter. There will be no clusters that overlap perfectly with any topic, but I can choose a few that correspond well. This is not a perfect method and there will be blind spots in coverage of each topic, but this is a low-stakes application so that's not too much of an issue. This will lead to most of the sentences being not being used in any topic filter whatsoever, but this isn't really a problem since the dataset is so large that each topic may still contain thousands of sentences for the user to learn from. When no filter is applied, I can show the user sentences chosen from the entire dataset.\n","\n","I'll plot the graph for 100 to 1000 clusters in increments of 100"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c72b26ab-4bef-44e6-b854-5bca3cd1f217","_uuid":"992bea80b2647c4f4e564bb020ce8eab07db6b78","collapsed":true,"scrolled":true,"trusted":false},"outputs":[],"source":["from sklearn.cluster import KMeans\n","\n","wcss = []\n","\n","for i in range(100,1001, 100):\n","\n","    # k-means++: initialization method selects initial cluster centroids that increase convergence speed\n","    # max_iter: max number of iterations for single run of k-means\n","    # n_init: The number of times k-means will be run with different centroid seeds. Final result taken\n","    # as the best output in terms of inertia. Since we're running k means lots of times and only plotting\n","    # a rough graph, n_init can be lower here to save time\n","    # random_state: Controls random number generator used for centroid initialization.\n","    # Setting to 0 seeds the random number generator with a fixed value, ensuring reproducibility.\n","    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)\n","    kmeans.fit(X3)\n","    # Append the inertia for this cluster number (inertia is just the within-cluster sum of squares)\n","    # to list of wcss for all cluster numbers\n","    wcss.append(kmeans.inertia_)\n","\n","# Plot wcss against num clusters\n","plt.plot(range(1,1001),wcss)\n","plt.xlabel('Number of clusters')\n","plt.ylabel('WCSS')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["As is to be expected, for such a large number of datapoints and clusters, the decrease in wcss with new clusters begins to resemble a smooth, exponentially decreasing curve.\n","\n","Looking at the plot in wcss_clusters.csv, I'll assume the y intercept y0 is around 900,000. Since wcss=665,000 when num clusters c = 1000, and exponential decay is modelled as f(c) = y0*e^(-c/œÑ), the curve can be modelled by calculating tau.\n","\n","If you rearrange the above equation you get œÑ = c/ln(y0/y) so œÑ=(1000/ln(900,000/665,000)) which roughly gives œÑ=3304.\n","\n","But looking at other data points, f(c) doesn't seem to behave like a standard exponentially decaying function, since œÑ doesn't stay constant as c increases. In fact, every time c increases by 200, œÑ seems to increase by a factor of around 1.2. Assuming this were a general rule, you could then say that œÑ and c are related by the following equation: œÑ = k * 1.2^(c/200).\n","\n","This means that the actual equation could be f(c) = y0^e-(c/(k * 1.2^(c/200)))\n","\n","And we can find k by substituting in wcss = 665,000, y0 = 900,000 and c = 1000 to the above equation gives k = 551.\n","\n","This is obviously based on a very small number of observations but it should give a reasonable estimate for the function linking wcss and number of clusters without having to actually run more simulations, since it already took 2 days to generate that chart.\n","\n","I'd say a good number of clusters to choose might be the point when wcss decreases by less than 5% by adding another 100 clusters:\n","\n","(f(c+100)-f(c))/f(c) <= 0.05\n","\n","According to this arbitrary cut-off point, and assuming my equation is a reasonable model for the relationship between wcss and c, this gives an optimal cluster number of 1386, which I'll round to 1400.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e096b262-a06c-4f0a-9c50-2ef4bda9b926","_uuid":"0ed982322b3a0fecb997e88ef0fb2f681c5a801c","collapsed":true,"scrolled":false,"trusted":false},"outputs":[],"source":["print(words[300:500])"]},{"cell_type":"markdown","metadata":{"_cell_guid":"0ca23daa-681a-4cb8-93ae-d2d2ac137604","_uuid":"f463135cb544f6ffa999861c7bf049cbbede8fe6"},"source":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"bff92434-ec9d-4e6b-a4a2-72ec4ea6f3bc","_uuid":"7225da580975f8589615a8fd5232ba3c80780845"},"source":["# 700 Clusters\n","\n","Now let's fit a k means with our optimal number of clusters"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4134a6d2-09aa-4cf5-9821-d95d0482c4ac","_uuid":"cf3c5af56aa05f6679effe85774207fe824255a1","collapsed":true,"trusted":false},"outputs":[],"source":["# Run with more initial seeds 20 to increase the odds of finding good centroids\n","# Finally, we look at 8 the clusters generated by k-means.\n","kmeans = KMeans(n_clusters = 8, n_init = 20)\n","kmeans.fit(X3)\n","\n","# Display 100 most common words in each cluster\n","# sort the feature values of all cluster centers in ascending order using the argsort method, and then take the last 100 indices\n","\"\"\"common_words = kmeans.cluster_centers_.argsort()[:,-1:-101:-1]\n","for num, centroid in enumerate(common_words):\n","    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["And print some sentences in each cluster"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","# Assign each sentence to a cluster\n","labels = kmeans.predict(X3)\n","\n","# Group sentences by cluster\n","clusters = {}\n","for i, label in enumerate(labels):\n","    if label not in clusters:\n","        clusters[label] = []\n","    clusters[label].append(sentences[i])\n","\n","# Print sentences in each cluster\n","for label, cluster_sentences in clusters.items():\n","    print(f'Cluster {label}:')\n","    for sentence in cluster_sentences[:10]:\n","        print(f'  - {sentence}')\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["# Save model and print out most common words for each cluster to file\n","\n","For each sentence, I'll add a new column in the database to indicate which cluster it belongs to. Then, once I have identified which clusters correcspond to which topics, I can simply filter the sentences accordingly when the user selects that topic.\n","\n","Saving the model means I can reuse it later to predict which cluster new sentences will be in without having to re-run the entire model, which would lead to different clusters and require me to redefine which clusters correspond to which topic, and then update the database, which would be tedious."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","\n","# Fitted scikit-learn models can be saved using pickle\n","with open(\"kmeans.pkl\", \"wb\") as f:\n","    pickle.dump(kmeans, f)\n","\n","# Save most common words for each cluster in a txt a file\n","common_words = kmeans.cluster_centers_.argsort()[:,-1:-101:-1]\n","\n","with open(\"cluster_words.txt\", \"w\") as f:\n","\n","    for num, centroid in enumerate(common_words):\n","        f.write(\"Cluster {}: {}\\n\".format(num + 1, \", \".join(words[word] for word in centroid)))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":1}
