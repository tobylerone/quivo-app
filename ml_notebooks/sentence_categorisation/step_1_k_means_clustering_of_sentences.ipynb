{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"8b9d6583-6ae1-4bb6-970c-c80d42dfebdb","_uuid":"d56b7961b91d7f67554ec48bd1d2485fb7419120"},"source":["# Sentence categorisation using TF-IDF with k-means\n","\n","The dataset was found here: https://wortschatz.uni-leipzig.de/en/download/French\n","It contains a mix of one million french sentences from various sources.\n","\n","Some alternatives: https://www.twine.net/blog/top-french-language-datasets/\n"]},{"cell_type":"markdown","metadata":{},"source":["TODO: Look at cluster sizes. If there are large differences it might be better to use another classification model like gaussian mixture\n","\n","The app will have many sentences in the target language that the user will see one after the other in increasing difficulty order. However, it would be nice if the user had the option to filter down the sentences they see to certain categories. For example, if they would like to practice vocabulary specific to the home, or sports. Instead of manually categorising what will eventually be thousands of sentences, I need to create a ml model to automatically place new sentences into a category as accurately as possible.\n","\n","There are a few supervised methods I could use such as naive bayes, support vector machines as well as deep learning models. However, I'll probably get the best results by using google's pre-trained BERT model as a base to encode the sentences and training a few fully-connected layers at the end for my custom classifyer. Computational cost is not so much of a concern as I'll only be running the classifier once on the dataset to generate a category column for the database. Although BERT was originally trained on an english corpus, there's another model called Multilingual BERT that has been trained on Wikipedia in 104 different languages, which would be well-suited to this application.\n","\n","However, I found it pretty hard to find a labeled ground-truth dataset for sentence-level subject classification, especially for foreign languages. I could create one myself but making an unbiased labelled dataset with enough samples in each category would be pretty difficult and probably not worth the time investment in this case, so in the end I've decided to use an unsupervised approach and infer categories. I need to pick between clustering and topic modelling.\n","\n","For this application, I want the sentences to be clustered by topic and not something else like sentiment. Representing the sentences using a bag-of-words model would group sentences together based on the words they contain, which would be more likely to result in topic-based clusters. Better yet, I could use TF-IDF, which, like bag-of-words, assigns a score based on the number of times a word appears in the document but is counterbalanced by the number of sentences in which it is present. The main downside of TF-IDF is that it doesn't account for document length when counting word frequency within documents. Consider using BM25. Words like ‚Äòthis‚Äô, ‚Äôare‚Äô, that are commonly present in all the sentences are not given a very high rank. However, a word that is present too many times in a few of the sentences will be given a higher rank as it might be indicative of its context.\n","\n","Once I have fit the k-means model I can use it to predict which category new sentences belong to.\n","\n","One obvious problem with this approach is that each cluster's sentences must be an almost perfect subset of a topic for it to be a useful filter. If a cluster's sentences talk about fashion 80% of the time but totally unrelated topics the rest of the time then it won't be of any use. I presume this will become less of a concern the greater the number of clusters.\n","\n","Alternative (less complicated) solution:\n","\n","I could also just make a big list of keywords for each topic and count how many words from each topic are in a phrase, then if it contains a certain nominal count / percentage of its words belonging to a certain topic(s), assign the sentence to the topic."]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":false},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to C:\\Users\\Toby\n","[nltk_data]     Usher\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import os\n","import re\n","import math\n","import numpy as np\n","import pandas as pd \n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","#from sklearn.feature_extraction import text\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.stem.snowball import SnowballStemmer\n","%matplotlib inline\n","\n","import constants\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Toby Usher\\AppData\\Local\\Temp\\ipykernel_6212\\741519457.py:6: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(filepath, delimiter='\\t', header=None)\n"]}],"source":["# Train on cleaned dataset of around 850,000 french sentences, capped to 200 characters max\n","filepath = os.path.join(f'../input_files/{constants.language_code}/sentences.csv')\n","\n","df = pd.read_csv(filepath, delimiter='\\t', header=None)\n","df.columns = [\"id\", \"sentence\"]"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 850172 entries, 0 to 850171\n","Data columns (total 2 columns):\n"," #   Column    Non-Null Count   Dtype \n","---  ------    --------------   ----- \n"," 0   id        850172 non-null  object\n"," 1   sentence  850172 non-null  object\n","dtypes: object(2)\n","memory usage: 13.0+ MB\n"]}],"source":["df.info()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>id</td>\n","      <td>sentence</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3</td>\n","      <td>La vierge du Seigneur s¬íen alla ensuite termin...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4</td>\n","      <td>Pr√©sentation de celui-ci √† plusieurs √©coles pr...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5</td>\n","      <td>Car chez le tout-petit, une diarrh√©e aigu√´ n'e...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6</td>\n","      <td>Le crat√®re Pwyll est un crat√®re d'impact situ√©...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id                                           sentence\n","0  id                                           sentence\n","1   3  La vierge du Seigneur s¬íen alla ensuite termin...\n","2   4  Pr√©sentation de celui-ci √† plusieurs √©coles pr...\n","3   5  Car chez le tout-petit, une diarrh√©e aigu√´ n'e...\n","4   6  Le crat√®re Pwyll est un crat√®re d'impact situ√©..."]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"f1bb8d35-27aa-4ff4-9a39-4329517aa6a4","_uuid":"f022fdf6441499ed52b34c063240f4f28b2ff3a5"},"source":["# NLP "]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['serais', 'mon', 't', '?', 'ayante', 'me', 'je', 'une', '√©tants', ')', 'eux', 'eussions', 'ai', '}', 'sera', ',', 'eurent', 'de', 'fussions', 'tes', 'sa', '√™tes', 'le', 'j', 'd', 'aurions', 'serai', 'avez', 'e√ªmes', 'moi', 'eussiez', 'm', 'eusse', 'ait', '√©taient', 'par', '√©t√©es', 'ses', 'eues', 'en', 'te', 'ayantes', 'avons', 'fus', '√©tant', 'eu', 'l', 's', 'ou', 'es', 'lui', 'f√ªmes', '√†', 'ayants', 'tu', 'avions', ';', '{', 'aviez', 'y', 'seraient', 'fut', 'fussiez', 'aient', 'eus', 'nos', 'soyons', 'as', '√©t√©', 'elle', 'f√ªt', 'leur', 'sont', 'ayez', 'avais', 'aurez', 'qu', 'ces', ':', 'nous', 'fusses', 'sois', '√©tantes', 'avaient', 'dans', 'avait', 'des', '√©tante', 'serait', '√©t√©e', 'sommes', 'au', 'pour', '√©tiez', 'aie', 'eue', '[', 'notre', 'e√ªt', 'ne', 'toi', 'f√ªtes', 'n', '√©t√©s', 'serons', 'fusse', 'du', 'c', 'e√ªtes', '.', 'ton', 'ce', 'ont', 'eussent', 'son', 'soit', 'vous', 'ayant', 'eusses', 'pas', 'que', '√©tions', 'soient', '(', 'suis', 'vos', 'il', 'avec', 'seriez', \"'\", 'serions', 'soyez', 'aurai', 'fussent', 'on', 'est', 'aura', 'et', 'auriez', 'mes', 'la', '√©tait', ']', 'se', 'sur', 'ma', 'les', '\"', 'eut', 'ayons', '!', 'm√™me', 'ta', 'aux', '√©tais', 'aurait', 'auront', 'serez', 'auraient', 'auras', 'seront', 'aurons', 'furent', 'un', 'qui', 'seras', 'aies', 'ils', '%', 'votre', 'aurais', 'mais']\n"]}],"source":["# First we need to get a list of stop words in french. These are common filler words that provide almost no useful information\n","# and occur frequently in most documents. Stop words can have a disproportionate influence on the overall representation of\n","# the document, which can be detrimental to the performance of TF-IDF. To mitigate this we need to remove stop words before\n","# calculating TF-IDF vectors\n","\n","punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\n","\n","# Get stop words for given language\n","stop_words = {\n","    'fr': stopwords.words('french')\n","}[constants.language_code]\n","\n","# Combine with standard punctuation to get the comprehensive list of stop words. Convert stop words to a set\n","# to ensure no duplicates\n","stop_words = list(set(stop_words).union(punc))\n","\n","print(stop_words)"]},{"cell_type":"markdown","metadata":{},"source":["There are lots of possible numerical values that can occur within a text, which means that a large fraction of the unique values could be numerical or at least contain strings of numbers. This can add noise to the data and lead to a sparse dataset and affect model performance, so I use a custom token pattern that ignores all words containing any numerical value. I might modify this in the future as it excludes important information such as dates, but as an initial proof-of-concept it's probably not too much of an issue."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Get the raw sentences from the dataframe\n","sentences = df['sentence'].values\n","\n","# Create a custom token pattern that matches words containing only letters\n","# The pattern \\b\\w+\\b matches whole words, and [^\\W\\d_] matches any character that is not a non-word character, digit, or underscore\n","token_pattern = r'\\b[^\\W\\d_]+\\b'\n","\n","# Create the TF-IDF vectorizer object and fit to our dataset to generate vector encodings for each sentence\n","vectorizer = TfidfVectorizer(stop_words=stop_words, token_pattern=token_pattern)\n","X = vectorizer.fit_transform(sentences)"]},{"cell_type":"code","execution_count":7,"metadata":{"_cell_guid":"eb56971e-5412-4138-a4cd-0e14844796be","_uuid":"10af64e15e2f08c30da71b847432eedc2aece199","collapsed":true,"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["281885\n","['a' 'aa' 'aaa' 'aaaa' 'aaaaa' 'aaaaaaaaahh' 'aaaaaah' 'aaaadddooorrre'\n"," 'aaaah' 'aaaatttttaaaaaaa' 'aaaf' 'aaahh' 'aaapnb' 'aaargghhh' 'aaargh'\n"," 'aaatchoum' 'aab' 'aabb' 'aabel' 'aabid' 'aaboubou' 'aac' 'aaccess'\n"," 'aaccessoirement' 'aach' 'aachen' 'aacom' 'aacr' 'aacs' 'aacsb' 'aad'\n"," 'aadaiss√©' 'aae' 'aaecc' 'aaeiae' 'aaeisep' 'aaequus' 'aaexa' 'aaf'\n"," 'aafanb' 'aafss' 'aage' 'aagje' 'aag√©e' 'aah' 'aahh' 'aai' 'aain'\n"," 'aaiodi' 'aaj' 'aak' 'aaker' 'aal' 'aalaadin' 'aaland' 'aalborg' 'aalis'\n"," 'aaliyah' 'aalst' 'aalstar' 'aalta' 'aalto' 'aaltra' 'aal√©nien'\n"," 'aal√©niens' 'aam' 'aames' 'aamodt' 'aams' 'aan' 'aanal' 'aanb' 'aang'\n"," 'aaos' 'aap' 'aapl' 'aapo' 'aapp' 'aappma' 'aapro' 'aapr√®s' 'aar' 'aarau'\n"," 'aarberg' 'aarch' 'aardman' 'aardmaniens' 'aardshock' 'aargauer' 'aarhus'\n"," 'aaricia' 'aarklash' 'aarniokoski' 'aarno' 'aaron' 'aarona' 'aaronson'\n"," 'aarosia' 'aarp' 'aarre']\n"]}],"source":["word_features = vectorizer.get_feature_names_out()\n","# Show the unique words that occurred in the training data\n","print(len(word_features))\n","print(word_features[:100])"]},{"cell_type":"markdown","metadata":{},"source":["It looks like the data could be further cleaned. For example, I could remove words with three or more of the same letter in a row. For now I'll leave it but I'll probably come back and properly clean the data once I have a preliminary output."]},{"cell_type":"markdown","metadata":{"_cell_guid":"871b1bd6-c411-4ff6-a784-9b376a0db4e6","_uuid":"e480f5b88938660f05c09f75af5f9f58d7110096"},"source":["# Stemming and Tokenizing\n","Stemming is the process of reducing a word into its stem, i.e. its root form. The root form is not necessarily a word by itself, but it can be used to generate words by concatenating the right suffix. For example, the words fish, fishes and fishing all stem into fish, which is a correct word. On the other side, the words study, studies and studying stems into studi, which is not an English word.\n","\n","Tokenization is breaking the sentence into words and punctuation,"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["language_name = {\n","    'fr': 'french'\n","}[constants.language_code]\n","\n","stemmer = SnowballStemmer(language_name)\n","# This is probably the wrong regex for french\n","tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n","\n","def _tokenize(text: str) -> list[str]:\n","    \"\"\"Tokenizes a document into its individual words and punctuation and returns a list of each token's stem.\n","    \"\"\"\n","\n","    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]"]},{"cell_type":"markdown","metadata":{"_cell_guid":"9877cf33-ebe8-46e4-b26e-c6f673617517","_uuid":"d190c4d6b9cb52ad70a021e3475704538b47f85f"},"source":["# Vectorization with stop words(words irrelevant to the model), stemming and tokenizing"]},{"cell_type":"code","execution_count":10,"metadata":{"_cell_guid":"18e1d30c-5515-4c0d-89e4-6af99658bee3","_uuid":"5fcc93fa3093f30d181ffa38b4ca46b133316952","collapsed":true,"scrolled":false,"trusted":false},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Toby Usher\\Documents\\dev\\quivo-app\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n","c:\\Users\\Toby Usher\\Documents\\dev\\quivo-app\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'f', 'fuss', 'fussion', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'taient', 'tais', 'tait', 'tant', 'ti', 'tion', 'votr'] not in stop_words.\n","  warnings.warn(\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Toby Usher\\Documents\\dev\\quivo-app\\ml_notebooks\\sentence_categorisation\\k_means_clustering_of_french_sentences.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m vectorizer2 \u001b[39m=\u001b[39m TfidfVectorizer(stop_words \u001b[39m=\u001b[39m stop_words, tokenizer \u001b[39m=\u001b[39m _tokenize)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X2 \u001b[39m=\u001b[39m vectorizer2\u001b[39m.\u001b[39;49mfit_transform(sentences)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m word_features2 \u001b[39m=\u001b[39m vectorizer2\u001b[39m.\u001b[39mget_feature_names_out()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(word_features2))\n","File \u001b[1;32mc:\\Users\\Toby Usher\\Documents\\dev\\quivo-app\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2119\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2120\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2121\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2122\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2123\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2124\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2125\u001b[0m )\n\u001b[1;32m-> 2126\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2127\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2128\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Toby Usher\\Documents\\dev\\quivo-app\\venv\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[1;32mc:\\Users\\Toby Usher\\Documents\\dev\\quivo-app\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1375\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1376\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1377\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1378\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1379\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1380\u001b[0m             )\n\u001b[0;32m   1381\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1383\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1385\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1386\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n","File \u001b[1;32mc:\\Users\\Toby Usher\\Documents\\dev\\quivo-app\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1269\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1270\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1271\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1272\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n","File \u001b[1;32mc:\\Users\\Toby Usher\\Documents\\dev\\quivo-app\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:112\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    110\u001b[0m     doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    111\u001b[0m \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 112\u001b[0m     doc \u001b[39m=\u001b[39m tokenizer(doc)\n\u001b[0;32m    113\u001b[0m \u001b[39mif\u001b[39;00m ngrams \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m stop_words \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","\u001b[1;32mc:\\Users\\Toby Usher\\Documents\\dev\\quivo-app\\ml_notebooks\\sentence_categorisation\\k_means_clustering_of_french_sentences.ipynb Cell 17\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_tokenize\u001b[39m(text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Tokenizes a document into its individual words and punctuation and returns a list of each token's stem.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [stemmer\u001b[39m.\u001b[39;49mstem(word) \u001b[39mfor\u001b[39;49;00m word \u001b[39min\u001b[39;49;00m tokenizer\u001b[39m.\u001b[39;49mtokenize(text\u001b[39m.\u001b[39;49mlower())]\n","\u001b[1;32mc:\\Users\\Toby Usher\\Documents\\dev\\quivo-app\\ml_notebooks\\sentence_categorisation\\k_means_clustering_of_french_sentences.ipynb Cell 17\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_tokenize\u001b[39m(text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Tokenizes a document into its individual words and punctuation and returns a list of each token's stem.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [stemmer\u001b[39m.\u001b[39;49mstem(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text\u001b[39m.\u001b[39mlower())]\n","File \u001b[1;32mc:\\Users\\Toby Usher\\Documents\\dev\\quivo-app\\venv\\Lib\\site-packages\\nltk\\stem\\snowball.py:2486\u001b[0m, in \u001b[0;36mFrenchStemmer.stem\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m step2a_success:\n\u001b[0;32m   2485\u001b[0m     \u001b[39mfor\u001b[39;00m suffix \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__step2b_suffixes:\n\u001b[1;32m-> 2486\u001b[0m         \u001b[39mif\u001b[39;00m rv\u001b[39m.\u001b[39mendswith(suffix):\n\u001b[0;32m   2487\u001b[0m             \u001b[39mif\u001b[39;00m suffix \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mions\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mions\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m r2:\n\u001b[0;32m   2488\u001b[0m                 word \u001b[39m=\u001b[39m word[:\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m]\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["vectorizer2 = TfidfVectorizer(stop_words = stop_words, tokenizer = _tokenize)\n","X2 = vectorizer2.fit_transform(sentences)\n","\n","word_features2 = vectorizer2.get_feature_names_out()\n","print(len(word_features2))\n","print(word_features2[:100]) "]},{"cell_type":"code","execution_count":11,"metadata":{"_cell_guid":"acdc11f4-7b5c-4aee-8c42-1a752bffbc2e","_uuid":"244a3015e5b4f4f84c174586fa875f5cf49cff1d","collapsed":true,"trusted":false},"outputs":[],"source":["vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = _tokenize, max_features = 10000) # limit to 10000 most frequent terms in the corpus\n","X3 = vectorizer3.fit_transform(sentences)\n","words = vectorizer3.get_feature_names_out()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"e8a8c1ed-8970-49e0-bae5-0e0d85abea84","_uuid":"b5f2e66c25d17527b78ae1a1fe174f1ac6310286"},"source":["# K-means clustering"]},{"cell_type":"markdown","metadata":{"_cell_guid":"c9a1312e-45f7-44e4-9407-13012bdf97ce","_uuid":"39ba3fa0f53454111495da2f7e7719572afec93a"},"source":["We can choose the appropriate number of clusters with the elbow method, which just plots the percentage of variance explained (ratio of the between-group variance to the total variance) as a function of the number of clusters. The appropriate cluster number is one where adding another cluster doesn't give much better modelling of the data i.e. the marginal gains of adding more clusters begins to drop off and doesn't provide a significantly better classification of the data. This point can be identified by an angle (elbow) in the graph, although this point isn't always easy to identify.\n","\n","A rule of thumb to estimate the value of k to choose is ùëò‚àºSQRT(n/2) where n is the number of points to be clustered."]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The recommended value of k when clustering 850172 data points is 652\n"]}],"source":["# Get the number of data points to be clustered\n","num_data_points = X3.shape[0]\n","recommended_k = math.ceil(math.sqrt(num_data_points/2))\n","\n","print(f\"The recommended value of k when clustering {num_data_points} data points is {recommended_k}\")"]},{"cell_type":"markdown","metadata":{},"source":["This is obviously far greater than the 10-15 sentence topic categories I would like to use in my app, so the elbow method will be ineffective as choosing values of k between say five and 20 should result in a relatively inverse linear relationship between k and wcss (within-cluster sum of squares).\n","\n","The purpose of the topic filter is to choose sentences that strongly match a specific topic, and choosing low k values will lead to extremely general categories, which is obviously not what I want.\n","\n","One solution is to choose a high value for k, say several hundred, and find clusters that are strongly correlated with a particular topic that I want to include as a filter. There will be no clusters that overlap perfectly with any topic, but I can choose a few that correspond well. This is not a perfect method and there will be blind spots in coverage of each topic, but this is a low-stakes application so that's not too much of an issue. This will lead to most of the sentences being not being used in any topic filter whatsoever, but this isn't really a problem since the dataset is so large that each topic may still contain thousands of sentences for the user to learn from. When no filter is applied, I can show the user sentences chosen from the entire dataset.\n","\n","I'll plot the graph for 100 to 1000 clusters in increments of 100"]},{"cell_type":"code","execution_count":15,"metadata":{"_cell_guid":"c72b26ab-4bef-44e6-b854-5bca3cd1f217","_uuid":"992bea80b2647c4f4e564bb020ce8eab07db6b78","collapsed":true,"scrolled":true,"trusted":false},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Toby Usher\\Documents\\dev\\quivo-app\\ml_notebooks\\sentence_categorisation\\k_means_clustering_of_french_sentences.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m,\u001b[39m1001\u001b[39m, \u001b[39m100\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# k-means++: initialization method selects initial cluster centroids that increase convergence speed\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#X31sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# random_state: Controls random number generator used for centroid initialization.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#X31sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# Setting to 0 seeds the random number generator with a fixed value, ensuring reproducibility.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#X31sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     kmeans \u001b[39m=\u001b[39m KMeans(n_clusters\u001b[39m=\u001b[39mi,init\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mk-means++\u001b[39m\u001b[39m'\u001b[39m,max_iter\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m,n_init\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#X31sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     kmeans\u001b[39m.\u001b[39;49mfit(X3)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#X31sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# Append the inertia for this cluster number (inertia is just the within-cluster sum of squares)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#X31sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# to list of wcss for all cluster numbers\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Toby%20Usher/Documents/dev/quivo-app/ml_notebooks/sentence_categorisation/k_means_clustering_of_french_sentences.ipynb#X31sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     wcss\u001b[39m.\u001b[39mappend(kmeans\u001b[39m.\u001b[39minertia_)\n","File \u001b[1;32mc:\\Users\\Toby Usher\\Documents\\dev\\quivo-app\\venv\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[1;32mc:\\Users\\Toby Usher\\Documents\\dev\\quivo-app\\venv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1515\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1511\u001b[0m best_inertia, best_labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_init):\n\u001b[0;32m   1514\u001b[0m     \u001b[39m# Initialize centers\u001b[39;00m\n\u001b[1;32m-> 1515\u001b[0m     centers_init \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_centroids(\n\u001b[0;32m   1516\u001b[0m         X,\n\u001b[0;32m   1517\u001b[0m         x_squared_norms\u001b[39m=\u001b[39;49mx_squared_norms,\n\u001b[0;32m   1518\u001b[0m         init\u001b[39m=\u001b[39;49minit,\n\u001b[0;32m   1519\u001b[0m         random_state\u001b[39m=\u001b[39;49mrandom_state,\n\u001b[0;32m   1520\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1521\u001b[0m     )\n\u001b[0;32m   1522\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n\u001b[0;32m   1523\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInitialization complete\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\Toby Usher\\Documents\\dev\\quivo-app\\venv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1018\u001b[0m, in \u001b[0;36m_BaseKMeans._init_centroids\u001b[1;34m(self, X, x_squared_norms, init, random_state, init_size, n_centroids, sample_weight)\u001b[0m\n\u001b[0;32m   1015\u001b[0m     sample_weight \u001b[39m=\u001b[39m sample_weight[init_indices]\n\u001b[0;32m   1017\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(init, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m init \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mk-means++\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 1018\u001b[0m     centers, _ \u001b[39m=\u001b[39m _kmeans_plusplus(\n\u001b[0;32m   1019\u001b[0m         X,\n\u001b[0;32m   1020\u001b[0m         n_clusters,\n\u001b[0;32m   1021\u001b[0m         random_state\u001b[39m=\u001b[39;49mrandom_state,\n\u001b[0;32m   1022\u001b[0m         x_squared_norms\u001b[39m=\u001b[39;49mx_squared_norms,\n\u001b[0;32m   1023\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1024\u001b[0m     )\n\u001b[0;32m   1025\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(init, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m init \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrandom\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1026\u001b[0m     seeds \u001b[39m=\u001b[39m random_state\u001b[39m.\u001b[39mchoice(\n\u001b[0;32m   1027\u001b[0m         n_samples,\n\u001b[0;32m   1028\u001b[0m         size\u001b[39m=\u001b[39mn_clusters,\n\u001b[0;32m   1029\u001b[0m         replace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1030\u001b[0m         p\u001b[39m=\u001b[39msample_weight \u001b[39m/\u001b[39m sample_weight\u001b[39m.\u001b[39msum(),\n\u001b[0;32m   1031\u001b[0m     )\n","File \u001b[1;32mc:\\Users\\Toby Usher\\Documents\\dev\\quivo-app\\venv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:255\u001b[0m, in \u001b[0;36m_kmeans_plusplus\u001b[1;34m(X, n_clusters, x_squared_norms, sample_weight, random_state, n_local_trials)\u001b[0m\n\u001b[0;32m    252\u001b[0m np\u001b[39m.\u001b[39mclip(candidate_ids, \u001b[39mNone\u001b[39;00m, closest_dist_sq\u001b[39m.\u001b[39msize \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, out\u001b[39m=\u001b[39mcandidate_ids)\n\u001b[0;32m    254\u001b[0m \u001b[39m# Compute distances to center candidates\u001b[39;00m\n\u001b[1;32m--> 255\u001b[0m distance_to_candidates \u001b[39m=\u001b[39m _euclidean_distances(\n\u001b[0;32m    256\u001b[0m     X[candidate_ids], X, Y_norm_squared\u001b[39m=\u001b[39;49mx_squared_norms, squared\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m    257\u001b[0m )\n\u001b[0;32m    259\u001b[0m \u001b[39m# update closest distances squared and potential for each candidate\u001b[39;00m\n\u001b[0;32m    260\u001b[0m np\u001b[39m.\u001b[39mminimum(closest_dist_sq, distance_to_candidates, out\u001b[39m=\u001b[39mdistance_to_candidates)\n","File \u001b[1;32mc:\\Users\\Toby Usher\\Documents\\dev\\quivo-app\\venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:382\u001b[0m, in \u001b[0;36m_euclidean_distances\u001b[1;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[0;32m    380\u001b[0m     distances \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m XX\n\u001b[0;32m    381\u001b[0m     distances \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m YY\n\u001b[1;32m--> 382\u001b[0m np\u001b[39m.\u001b[39;49mmaximum(distances, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mdistances)\n\u001b[0;32m    384\u001b[0m \u001b[39m# Ensure that distances between vectors and themselves are set to 0.0.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[39m# This may not be the case due to floating point rounding errors.\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m Y:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from sklearn.cluster import KMeans\n","\n","wcss = []\n","\n","for i in range(100,1001, 100):\n","\n","    # k-means++: initialization method selects initial cluster centroids that increase convergence speed\n","    # max_iter: max number of iterations for single run of k-means\n","    # n_init: The number of times k-means will be run with different centroid seeds. Final result taken\n","    # as the best output in terms of inertia. Since we're running k means lots of times and only plotting\n","    # a rough graph, n_init can be lower here to save time\n","    # random_state: Controls random number generator used for centroid initialization.\n","    # Setting to 0 seeds the random number generator with a fixed value, ensuring reproducibility.\n","    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)\n","    kmeans.fit(X3)\n","    # Append the inertia for this cluster number (inertia is just the within-cluster sum of squares)\n","    # to list of wcss for all cluster numbers\n","    wcss.append(kmeans.inertia_)\n","\n","# Plot wcss against num clusters\n","plt.plot(range(1,1001),wcss)\n","plt.xlabel('Number of clusters')\n","plt.ylabel('WCSS')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["As is to be expected, for such a large number of datapoints and clusters, the decrease in wcss with new clusters begins to resemble a smooth, exponentially decreasing curve.\n","\n","Looking at the plot in wcss_clusters.csv, I'll assume the y intercept y0 is around 900,000. Since wcss=665,000 when num clusters c = 1000, and exponential decay is modelled as f(c) = y0*e^(-c/œÑ), the curve can be modelled by calculating tau.\n","\n","If you rearrange the above equation you get œÑ = c/ln(y0/y) so œÑ=(1000/ln(900,000/665,000)) which roughly gives œÑ=3304.\n","\n","But looking at other data points, f(c) doesn't seem to behave like a standard exponentially decaying function, since œÑ doesn't stay constant as c increases. In fact, every time c increases by 200, œÑ seems to increase by a factor of around 1.2. Assuming this were a general rule, you could then say that œÑ and c are related by the following equation: œÑ = k * 1.2^(c/200).\n","\n","This means that the actual equation could be f(c) = y0^e-(c/(k * 1.2^(c/200)))\n","\n","And we can find k by substituting in wcss = 665,000, y0 = 900,000 and c = 1000 to the above equation to get k = 551.\n","\n","This is obviously based on a very small number of observations but it should give a reasonable estimate for the function linking wcss and number of clusters without having to actually run more simulations, since it already took 2 days to generate that chart.\n","\n","I'd say a good number of clusters to choose might be the point when wcss decreases by less than 5% by adding another 100 clusters:\n","\n","(f(c+100)-f(c))/f(c) <= 0.05\n","\n","According to this arbitrary cut-off point, and assuming my equation is a reasonable model for the relationship between wcss and c, this gives an optimal cluster number of 1386, which I'll round to 1400.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"_cell_guid":"e096b262-a06c-4f0a-9c50-2ef4bda9b926","_uuid":"0ed982322b3a0fecb997e88ef0fb2f681c5a801c","collapsed":true,"scrolled":false,"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["['amend' 'amer' 'americ' 'american' 'ami' 'amical' 'amien' 'amin' 'amiral'\n"," 'amis' 'amit' 'amnesty' 'amont' 'amorc' 'amort' 'amour' 'ample' 'ampleur'\n"," 'amplif' 'ampoul' 'amput' 'amro' 'amsterdam' 'amus' 'an' 'ana' 'analog'\n"," 'analogu' 'analy' 'analys' 'analyst' 'analyt' 'anarch' 'anatol' 'anatom'\n"," 'anc' 'ance' 'ancestral' 'ancien' 'ancier' 'ancr' 'and' 'anderlecht'\n"," 'anderson' 'andr' 'andre' 'andrei' 'andrew' 'andy' 'anecdot' 'anesth'\n"," 'ang' 'ange' 'angel' 'anger' 'anglais' 'angle' 'angleterr' 'anglo'\n"," 'anglophon' 'angoiss' 'angol' 'anim' 'animal' 'aniqu' 'ankar' 'anmoin'\n"," 'ann' 'anna' 'annal' 'annan' 'anne' 'anneau' 'annex' 'anni' 'anniversair'\n"," 'annon' 'annonc' 'annoncent' 'annonceur' 'annuair' 'annuel' 'annul'\n"," 'anomal' 'anonym' 'anormal' 'ant' 'ante' 'anten' 'anthony' 'anthropolog'\n"," 'anti' 'antibiot' 'anticip' 'anticorp' 'antidopag' 'antill' 'antiqu'\n"," 'antis' 'antiterror' 'antoin' 'anton' 'antonio' 'anver' 'ao' 'aol' 'ap'\n"," 'apach' 'apais' 'aper' 'api' 'apog' 'apostol' 'app' 'appar' 'apparaiss'\n"," 'apparaissent' 'appareil' 'apparent' 'apparit' 'appart' 'apparten'\n"," 'appartiennent' 'appartient' 'apparu' 'apparus' 'appel' 'appellent'\n"," 'appelon' 'applaud' 'apple' 'applic' 'appliqu' 'apport' 'apportent'\n"," 'appos' 'appr' 'appren' 'apprend' 'apprendr' 'apprennent' 'apprent'\n"," 'apprentissag' 'appris' 'approb' 'approch' 'approfond' 'appropr'\n"," 'appropri' 'approuv' 'approvision' 'approxim' 'appui' 'appuis' 'appuy'\n"," 'apr' 'apre' 'apte' 'aptitud' 'aquarium' 'aquat' 'aquitain' 'ar' 'arab'\n"," 'arbitr' 'arbitrag' 'arbitrair' 'arbor' 'arbre' 'arc' 'arcad' 'arcelor'\n"," 'arch' 'archa' 'archer' 'archev' 'architect' 'architectur'\n"," 'architectural' 'archiv' 'ard' 'arden' 'ardent' 'are' 'aren' 'arev'\n"," 'arfa' 'argent' 'argentin' 'argil' 'argu' 'argument' 'ari' 'arian'\n"," 'aristocrat' 'arm' 'armand' 'arme' 'armement' 'armoir']\n"]}],"source":["print(words[300:500])"]},{"cell_type":"markdown","metadata":{"_cell_guid":"bff92434-ec9d-4e6b-a4a2-72ec4ea6f3bc","_uuid":"7225da580975f8589615a8fd5232ba3c80780845"},"source":["# 1400 Clusters\n","\n","Now let's fit a k means with our chosen number of clusters"]},{"cell_type":"code","execution_count":21,"metadata":{"_cell_guid":"4134a6d2-09aa-4cf5-9821-d95d0482c4ac","_uuid":"cf3c5af56aa05f6679effe85774207fe824255a1","collapsed":true,"trusted":false},"outputs":[],"source":["# Run with more initial seeds 20 to increase the odds of finding good centroids\n","kmeans = KMeans(n_clusters = 1400, n_init = 20)\n","kmeans.fit(X3)\n","\n","# Display 100 most common words in each cluster\n","# sort the feature values of all cluster centers in ascending order using the argsort method, and then take the last 100 indices\n","\"\"\"common_words = kmeans.cluster_centers_.argsort()[:,-1:-101:-1]\n","for num, centroid in enumerate(common_words):\n","    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))\n","\"\"\"\n","\n","import pickle\n","\n","# Fitted scikit-learn models can be saved using pickle\n","with open(f\"../output_files/{constants.language_code}/step1_kmeans.pkl\", \"wb\") as f:\n","    pickle.dump(kmeans, f)\n","\n","# Save most common words for each cluster in a txt a file\n","common_words = kmeans.cluster_centers_.argsort()[:,-1:-101:-1]\n","\n","with open(f\"../output_files/{constants.language_code}/step1_cluster_words.txt\", \"w\") as f:\n","\n","    for num, centroid in enumerate(common_words):\n","        f.write(\"Cluster {}: {}\\n\".format(num + 1, \", \".join(\n","            words[word] for word in centroid)))"]},{"cell_type":"markdown","metadata":{},"source":["And print some sentences in each cluster"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","# Assign each sentence to a cluster\n","labels = kmeans.predict(X3)\n","\n","# Group sentences by cluster\n","clusters = {}\n","for i, label in enumerate(labels):\n","    if label not in clusters:\n","        clusters[label] = []\n","    clusters[label].append(sentences[i])\n","\n","# Print sentences in each cluster\n","for label, cluster_sentences in clusters.items():\n","    print(f'Cluster {label}:')\n","    for sentence in cluster_sentences[:10]:\n","        print(f'  - {sentence}')\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["# Save model and print out most common words for each cluster to file\n","\n","For each sentence, I'll add a new column in the database to indicate which cluster it belongs to. Then, once I have identified which clusters correcspond to which topics, I can simply filter the sentences accordingly when the user selects that topic.\n","\n","Saving the model means I can reuse it later to predict which cluster new sentences will be in without having to re-run the entire model, which would lead to different clusters and require me to redefine which clusters correspond to which topic, and then update the database, which would be tedious."]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["import pickle\n","\n","# Fitted scikit-learn models can be saved using pickle\n","with open(f\"../output_files/{constants.language_code}/step1_kmeans.pkl\", \"wb\") as f:\n","    pickle.dump(kmeans, f)\n","\n","# Save most common words for each cluster in a txt a file\n","common_words = kmeans.cluster_centers_.argsort()[:,-1:-101:-1]\n","\n","with open(f\"../output_files/{constants.language_code}/step1_cluster_words.txt\", \"w\") as f:\n","\n","    for num, centroid in enumerate(common_words):\n","        f.write(\"Cluster {}: {}\\n\".format(num + 1, \", \".join(words[word] for word in centroid)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":1}
